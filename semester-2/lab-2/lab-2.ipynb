{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа № 2\n",
    "### Минимальная оценка - 3 балла\n",
    "### Максимальная оценка - 5 балла\n",
    "Целью данной работы является изучение метода градиентного спуска для минимизации эмпирического риска функции потерь логистической регресии и метода опорных векторов..\n",
    "Для успешной сдачи лабораторной работы Вам необходимо предоставить заполненый исходный ноутбук."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет для классификации IP-адресов («1», если с данного IP нужно запретить регистрации аккаунтов без привязки телефона, «0» - в противном случае)\n",
    "\n",
    "Признаки в файле feature_set.pickle:\n",
    "1. количество аккаунтов, зарегистрированных людьми за последний час\n",
    "2. количество аккаунтов, зарегистрированных скриптами за последний час\n",
    "3. количество зарегистрированных аккаунтов последний час, привязавших телефон\n",
    "\n",
    "Метки классов в файле label_set.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  6.,  2.,  2.],\n",
       "       [ 1.,  0.,  2.,  0.],\n",
       "       [ 1., 22.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 1.,  2.,  2.,  0.],\n",
       "       [ 1.,  0.,  2.,  0.],\n",
       "       [ 1.,  0.,  4.,  0.]], dtype=float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('feature_set.pickle', 'rb') as f:\n",
    "    X = pickle.load(f, encoding='latin-1') \\\n",
    "        .astype(np.longdouble)\n",
    "    \n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "n_feat = X.shape[1]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('label_set.pickle', 'rb') as f:\n",
    "    y = pickle.load(f, encoding='latin-1') \\\n",
    "        .astype(np.longdouble) \\\n",
    "        .reshape((-1, 1))\n",
    "    \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (3 балла)\n",
    "### Вариант 1\n",
    "#### Реализовать модель логистической регрессии и минимизировать её функцию потерь\n",
    "Логистическая регрессия применяется для прогнозирования вероятности возникновения некоторого события по значениям множества признаков. Для этого вводится так называемая зависимая переменная $y$, принимающая лишь одно из двух значений — как правило, это числа 0 (событие не произошло) и 1 (событие произошло), и множество независимых переменных (также называемых признаками, предикторами или регрессорами) — вещественных $x_{1},x_{2},...,x_{n}$, на основе значений которых требуется вычислить вероятность принятия того или иного значения зависимой переменной.\n",
    "\n",
    "Модель логистической регрессии задаётся формулой сигмоиды: $$\\vec{p}=\\frac{1}{1+e^{-\\vec{x}\\vec{w}}}$$\n",
    "где $\\vec{p}$ - вероятность наступления события $y=1$, $X$ - матрица признаков, $\\vec{w}$ - вектор весов.\n",
    "(Более подробно про логистическую регрессию можно почитать [тут](http://www.machinelearning.ru/wiki/index.php?title=Логистическая_регрессия))\n",
    "### Вариант 2\n",
    "#### Реализовать модель SVM и минимизировать её функцию потерь\n",
    "Основная идея SVM — перевод исходных векторов в пространство более высокой размерности и поиск разделяющей гиперплоскости с максимальным зазором в этом пространстве. Две параллельных гиперплоскости строятся по обеим сторонам гиперплоскости, разделяющей классы. Разделяющей гиперплоскостью будет гиперплоскость, максимизирующая расстояние до двух параллельных гиперплоскостей. Алгоритм работает в предположении, что чем больше разница или расстояние между этими параллельными гиперплоскостями, тем меньше будет средняя ошибка классификатора.\n",
    "\n",
    "Для предсказания SVM используется функция $sign$:\n",
    "$$sign(s)=\\left\\{\\begin{matrix}\n",
    "1, если &s>=0\n",
    "\\\\ \n",
    "-1, если &s<0\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "$$s=X\\vec{w}$$\n",
    "\n",
    "Отличие SVM от логистической регрессии в том, что метки класса должны принимать значения $\\{-1,1\\}$ (Более подробно про SVM можно почитать [тут](http://www.ccas.ru/voron/download/SVM.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вариант 1\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "V = (N % 2) + 1\n",
    "print(f'Вариант {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.1\n",
    "### Вариант 1\n",
    "Реализуйте функцию сигмоиды и функцию потерь для логистической регрессии, которая представлена формулой:\n",
    "$$LogLoss=-\\sum_{i=1}^{N}y_{i}log(p_{i})+(1-y_{i})log(1-p_{i})$$\n",
    "### Вариант 2\n",
    "Реализуте функцию потерь для SVM, которая представлена формулой:\n",
    "$$hinge\\_loss=\\frac{1}{n}\\sum_{i=1}^{n} max(0,1-predicted\\_value * true\\_value) $$\n",
    "$max(a,b)$ - возвращает наибольшее из двух чисел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sigmoid(features, weights):\n",
    "    return 1 / (1 + np.exp(-features.dot(weights)))\n",
    "\n",
    "def my_loss_function(predicted_values, true_values):\n",
    "    return -np.sum(true_values * np.log(predicted_values) + (1 - true_values) * np.log(1 - predicted_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.2\n",
    "Реализуйте (на свой выбор) функцию инициализации весов (см. лекции) и выведите результат для $length=N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_weights(length):\n",
    "    #\n",
    "    # Просто нули работают хорошо\n",
    "    #\n",
    "    \n",
    "    return np.zeros((length, 1)).astype(np.longdouble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.3\n",
    "### Вариант 1\n",
    "Реализуйте свою модель логистической регрессии, на вход которой подаётся матрица объект-признак и вектор весов.\n",
    "<br>На выходе возвращается предсказанная целевая переменная.\n",
    "<br>P.S.: Подумайте, как получить метки 0/1 из вероятностей сигмоиды, используя свой номер в группе.\n",
    "### Вариант 2\n",
    "Реализуйте свой SVM, на вход которой подаётся матрица объект-признак и вектор весов.\n",
    "<br>На выходе возвращается предсказанная целевая переменная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(features, weights):\n",
    "    return np.array([1 if p >= 0.5 else 0 for p in my_sigmoid(features, weights)]) \\\n",
    "        .reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.4\n",
    "Реализуйте метод стохастического градиентного спуска (см. лекции и ЛР-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_of_my_loss_func(X, y, weights):\n",
    "    return X.T.dot(my_sigmoid(X, weights) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weigths(grad, weights, learning_rate):\n",
    "    return weights - learning_rate * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_stochastic_grad_descent(X, y, initial_weights, iter_num, learning_rate=0.1):\n",
    "    weights = initial_weights\n",
    "    \n",
    "    for _ in range(iter_num):\n",
    "        weights = update_weigths(grad_of_my_loss_func(X, y, weights), \n",
    "                                 weights, \n",
    "                                 learning_rate)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.5\n",
    "1. Разделите свою выборку на обучающую и тестовую\n",
    "2. Сгенерируйте веса\n",
    "3. Используя свой метод стохастического градиентного спуска и свою модель, подберите оптимальные веса для обучающей выборки\n",
    "4. Используя полученные на шаге 3 веса сделайте предсказание для тестовой выборки\n",
    "5. Выведите значение функции потерь при изначально сгенерированных весах и при весах, полученных в результате градиентного спуска\n",
    "6. Выведите метрики качества для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 70\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = initial_weights(n_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6287.538074859263"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_loss_function(my_sigmoid(X_train, weights), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = my_stochastic_grad_descent(X_train, y_train, weights, \n",
    "                                     iter_num=n_iter, \n",
    "                                     learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53827022],\n",
       "       [0.53827022],\n",
       "       [0.80385464],\n",
       "       ...,\n",
       "       [0.49987302],\n",
       "       [0.44421484],\n",
       "       [0.44421484]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sigmoid(X_train, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5121.793142445854"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_loss_function(my_sigmoid(X_train, weights), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_model(X_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6965020576131687, 0.6811663479923518)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred), precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 (2 балла)\n",
    "### Вариант 1\n",
    "Реализуйте алгоритм логистической регрессии с возможностью выбора регуляризации ($L_{1}$ / $L_{2}$), обучите его и сравните результат с тем, что получилось в задании 1.5 (используя метрики качества).\n",
    "### Вариант 2\n",
    "Реализуйте алгоритм SVM с возможностью выбора регуляризации ($L_{1}$ / $L_{2}$), обучите его и сравните результат с тем, что получилось в задании 1.5 (используя метрики качества).\n",
    "<br>Про регуляризацию можно почитать [тут](https://learnmachinelearning.wikia.org/ru/wiki/Регуляризация)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_of_my_loss_func_advanced(X, y, weights, regularization='l2', l=0.5):\n",
    "    \n",
    "    def l1_grad_penalty(weights, l):\n",
    "        return l * np.sign(weights)\n",
    "\n",
    "    def l2_grad_penalty(weights, l):\n",
    "        return l * weights\n",
    "\n",
    "    def no_penaly(weights, l):\n",
    "        return np.zeros(weights.shape)\n",
    "    \n",
    "    reg_functions = {\n",
    "        'l1': l1_grad_penalty,\n",
    "        'l2': l2_grad_penalty\n",
    "    }\n",
    "    \n",
    "    penalty = reg_functions.get(regularization, no_penaly)\n",
    "    \n",
    "    if (0 > l or l > 1):\n",
    "        raise ValueError('l must be in range [0, 1], but equals {l}')\n",
    "    \n",
    "    return X.T.dot(my_sigmoid(X, weights) - y) + X.shape[0] * penalty(weights, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_stochastic_grad_descent_advanced(X, y, initial_weights, iter_num, \n",
    "                                        learning_rate=0.1, \n",
    "                                        regularization='l2', l=0.5):\n",
    "    weights = initial_weights\n",
    "    \n",
    "    for _ in range(iter_num):\n",
    "        weights = update_weigths(grad_of_my_loss_func_advanced(X, y, weights, regularization, l), \n",
    "                                 weights, \n",
    "                                 learning_rate)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-регуляризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = initial_weights(n_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = my_stochastic_grad_descent_advanced(X_train, y_train, weights, \n",
    "                                              iter_num=n_iter, \n",
    "                                              learning_rate=learning_rate, \n",
    "                                              regularization='l2', l=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5184.206477108912"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_loss_function(my_sigmoid(X_train, weights), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_model(X_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7044753086419753, 0.6754112939084037)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred), precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1-регуляризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = initial_weights(n_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = my_stochastic_grad_descent_advanced(X_train, y_train, weights, \n",
    "                                              iter_num=n_iter, \n",
    "                                              learning_rate=learning_rate, \n",
    "                                              regularization='l1', l=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5376.3791735663635"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_loss_function(my_sigmoid(X_train, weights), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_model(X_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6862139917695473, 0.6544750430292599)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred), precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
